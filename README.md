# DistributedCNN

This project aims at finding the better distributed training architecture between Parameter Server (Standard distributed TensorFlow) and Ring Allreduce (Horovod).

### **To Do:**
- Update the existing codes to Tensorflow 2.0+ version as most of the dependencies of the current implementation have been deprecated. The original version was written on Tensorflow 0.9
- Codes related to distributed training may not be upgraded in this branch.

### **Resources:**
- Book: _Deep leaning with Python_
- https://cs230.stanford.edu/blog/datapipeline/